# import re
# import os

# import pandas as pd


# class IemocapPreprocessor:
#     def __init__(self, dataset_path: str):
#         self._dataset_path = dataset_path

#         # self._info_line = re.compile(r"\[.+\]\n", re.IGNORECASE)
        
        
#     # bien doi tu wav ve text
#     # 1. dua qua phowhisper de phien am
#     # 2. dua qua photonx de sua loi
#     def transcript_data(self, audio_path: str) -> str:
#         import torch
#         from transformers import WhisperProcessor, WhisperForConditionalGeneration
#         import librosa
#         import numpy as np

#         # 1. Äá»‹nh nghÄ©a tÃªn mÃ´ hÃ¬nh PhoWhisper Small
#         model_name = "vinai/phowhisper-small"

#         # 2. Táº£i Processor vÃ  Model
#         # Processor sáº½ táº£i tokenizer vÃ  feature extractor
#         processor = WhisperProcessor.from_pretrained(model_name)
#         model = WhisperForConditionalGeneration.from_pretrained(model_name)

#         # 3. Chuyá»ƒn mÃ´ hÃ¬nh sang GPU náº¿u cÃ³
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         model.to(device) # type: ignore

#         # print(f"ÄÃ£ táº£i mÃ´ hÃ¬nh {model_name} vÃ  sá»­ dá»¥ng trÃªn thiáº¿t bá»‹: {device}")
        
#         def transcribe_vietnamese_audio(audio_path):
#             """
#             Chuyá»ƒn Ä‘á»•i tá»‡p Ã¢m thanh Tiáº¿ng Viá»‡t sang vÄƒn báº£n báº±ng PhoWhisper Small.

#             Args:
#                 audio_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n tá»‡p Ã¢m thanh (vÃ­ dá»¥: .wav, .mp3).

#             Returns:
#                 str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i.
#             """
#             # 1. Táº£i vÃ  láº¥y máº«u láº¡i (resample) tá»‡p Ã¢m thanh vá» táº§n sá»‘ 16kHz (yÃªu cáº§u cá»§a Whisper)
#             # librosa.load tráº£ vá» numpy array vÃ  sample rate (sr)
#             speech_array, sampling_rate = librosa.load(audio_path, sr=16000)

#             # 2. Xá»­ lÃ½ Ä‘áº§u vÃ o: trÃ­ch xuáº¥t features
#             # Cáº§n Ä‘áº£m báº£o ráº±ng audio lÃ  má»™t máº£ng 1D (mono)
#             input_features = processor(
#                 speech_array, 
#                 sampling_rate=sampling_rate, 
#                 return_tensors="pt"
#             ).input_features
            
#             # 3. Chuyá»ƒn input features sang thiáº¿t bá»‹ (GPU/CPU)
#             input_features = input_features.to(device)

#             # 4. Cháº¡y Inference (táº¡o vÄƒn báº£n)
#             with torch.no_grad():
#                 # Tham sá»‘ language="vi" vÃ  task="transcribe" lÃ  báº¯t buá»™c Ä‘á»‘i vá»›i PhoWhisper
#                 predicted_ids = model.generate(
#                     input_features, 
#                     forced_decoder_ids=processor.get_decoder_prompt_ids(language="vi", task="transcribe")
#                 )

#             # 5. Giáº£i mÃ£ ID thÃ nh chuá»—i vÄƒn báº£n
#             transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)[0]
            
#             return transcription
        
#         # def correct_transcription_with_photonx(transcription: str) -> str:
            
#         #     # pháº£i cÃ i pip install --upgrade photonx
#         #     from photonx import PhotonX
            
#         #     import os
#         #     os.environ["PROTONX_API_KEY"] = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6Im5ndXllbnZpZXRodW5nc29pY3RodXN0QGdtYWlsLmNvbSIsImlhdCI6MTc2NTY3Nzg0NywiZXhwIjoxNzY4MjY5ODQ3fQ.azkJoKM3TvwAsu_4NmPRifs8f9Vr8IXzMLP_GArNNxg"

#         #     # Khá»Ÿi táº¡o PhotonX vá»›i mÃ´ hÃ¬nh Tiáº¿ng Viá»‡t
#         #     photonx = PhotonX()

#         #     # Sá»­ dá»¥ng PhotonX Ä‘á»ƒ sá»­a lá»—i chÃ­nh táº£ vÃ  ngá»¯ phÃ¡p
#         #     corrected_text = photonx.text.correct(input=transcription,top_k=1)['data'][0]['candidates'][0]['output']

#         #     return corrected_text
        
#         transcription = transcribe_vietnamese_audio(audio_path)
#         # corrected_transcription = correct_transcription_with_photonx(transcription)
#         corrected_transcription = transcription
#         return corrected_transcription
        
    
#     def labeling_data(self,folder_path:str, label:str) -> pd.DataFrame:
#         import os
#         import pandas as pd
#         from typing import List
        
#         """
#         QuÃ©t qua thÆ° má»¥c, gÃ¡n nhÃ£n cá»‘ Ä‘á»‹nh cho táº¥t cáº£ cÃ¡c tá»‡p WAV, 
#         vÃ  tráº£ vá» má»™t DataFrame vá»›i cá»™t 'audio' (tÃªn tá»‡p khÃ´ng Ä‘uÃ´i) vÃ  'emotion'.

#         Args:
#             folder_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a tá»‡p WAV.
#             emotion_label (str): NhÃ£n cáº£m xÃºc (vÃ­ dá»¥: "anger") muá»‘n gÃ¡n.

#         Returns:
#             pd.DataFrame: DataFrame vá»›i hai cá»™t ('audio', 'emotion').
#         """
        
#         if not os.path.isdir(folder_path):
#             print(f"Lá»—i: ThÆ° má»¥c khÃ´ng tá»“n táº¡i táº¡i Ä‘Æ°á»ng dáº«n: {folder_path}")
#             return pd.DataFrame()

#         audio_names: List[str] = [] # Danh sÃ¡ch lÆ°u tÃªn tá»‡p khÃ´ng cÃ³ Ä‘uÃ´i

#         for file_name in os.listdir(folder_path):
#             # 1. Kiá»ƒm tra vÃ  lá»c tá»‡p WAV (khÃ´ng phÃ¢n biá»‡t chá»¯ hoa/thÆ°á»ng)
#             if file_name.lower().endswith('.wav'):
#                 # 2. XÃ³a Ä‘uÃ´i .wav khá»i tÃªn tá»‡p
#                 name_without_extension = file_name[:-4] 
#                 # Giáº£ sá»­ file luÃ´n káº¿t thÃºc báº±ng .wav (4 kÃ½ tá»±)
#                 audio_names.append(folder_path + name_without_extension)

#         if not audio_names:
#             print(f"Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y tá»‡p .wav nÃ o trong thÆ° má»¥c: {folder_path}")
#             return pd.DataFrame()
            
#         # 3. Táº¡o DataFrame vá»›i tÃªn cá»™t má»›i
#         data = {
#             'audio': audio_names,
#             'emotion': [label] * len(audio_names) # GÃ¡n nhÃ£n cá»‘ Ä‘á»‹nh
#         }
        
#         df = pd.DataFrame(data)
#         print(f"ÄÃ£ táº¡o DataFrame vá»›i {len(df)} hÃ ng.")
#         return df
        



#     def generate_dataframe(self) -> pd.DataFrame:
#         audios = []
#         emotions = []
#         texts = []

    
#         # read folder
#         sca_folder = os.path.join(self._dataset_path, "scare") #scare
#         sup_folder = os.path.join(self._dataset_path, "surprise") #surprise
        
#         # process labeling data
#         sca_df = self.labeling_data(sca_folder, "scare")
#         sup_df = self.labeling_data(sup_folder, "surprise")
        
        
#         # read metadata hap, neu, sad, ang
#         metadata_path = os.path.join(self._dataset_path,'happy+neutral+angry+sad' ,"metadata.csv")
#         metadata_df = pd.read_csv(metadata_path)
#         metadata_df['audio'] = metadata_df['filename'].str.replace('.wav', '', regex=False) # remove .wav extension
        
#         happy_neutral_angry_sad_path = os.path.join(self._dataset_path,'happy+neutral+angry+sad','converted' )
#         metadata_df['audio'] = happy_neutral_angry_sad_path + metadata_df['audio']
#         # metadata_df['audio'] = metadata_df['audio'].apply(lambda x: os.path.join(happy_neutral_angry_sad_path, x + '.wav'))
#         metadata_df = metadata_df[['audio','emotion']]
        
        
#         # combine all dataframe
#         data_df = pd.concat([sca_df, sup_df, metadata_df], ignore_index=True)
        
        
#         # process transcript data
#         for index, row in data_df.iterrows():
#             audio_path = row['audio'] + '.wav'  # ThÃªm Ä‘uÃ´i .wav vÃ o Ä‘Æ°á»ng dáº«n tá»‡p Ã¢m thanh
#             text = self.transcript_data(audio_path)
#             audios.append(audio_path)
#             texts.append(text)
#             emotions.append(row['emotion'])
#         # convert to dataframe and shuffle data
#         df_shuffled = pd.DataFrame(data={"audio": audios, "text": texts, "emotion": emotions}).sample(frac=1, random_state=42).reset_index(drop=True)

        
        

#         return df_shuffled


import os
import pandas as pd
from typing import List
from tqdm import tqdm


class IemocapPreprocessor:
    def __init__(
        self,
        dataset_path: str,
        batch_size: int = 12
    ):
        self.dataset_path = dataset_path
        self.batch_size = batch_size

        # ===== Load model 1 láº§n =====
        import torch
        from transformers import WhisperProcessor, WhisperForConditionalGeneration

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_name = "vinai/phowhisper-small"

        print(f"[INFO] Loading model {self.model_name} on {self.device}")

        self.processor = WhisperProcessor.from_pretrained(self.model_name)
        self.model = WhisperForConditionalGeneration.from_pretrained(self.model_name)
        self.model.to(self.device)
        self.model.eval()

        print("[INFO] Model loaded successfully")
        
    def labeling_data(self, folder_path: str, label: str) -> pd.DataFrame:
        audio_paths = []

        for f in os.listdir(folder_path):
            if f.lower().endswith(".wav"):
                audio_paths.append(
                    os.path.join(folder_path, f)  # bá» .wav
                )

        return pd.DataFrame({
            "audio": audio_paths,
            "emotion": label
        })

    # def batch_transcribe(self, audio_paths: List[str]) -> List[str]:
    #     import torch
    #     import torchaudio
    #     from torch.amp import autocast

    #     results = []

    #     forced_decoder_ids = self.processor.get_decoder_prompt_ids(
    #         language="vi",
    #         task="transcribe"
    #     )

    #     for i in tqdm(range(0, len(audio_paths), self.batch_size), desc="Transcribing"):
    #         batch_paths = audio_paths[i:i + self.batch_size]
    #         speeches = []

    #         for path in batch_paths:
    #             waveform, sr = torchaudio.load(path)
    #             waveform = waveform.mean(dim=0)  # mono
    #             if sr != 16000:
    #                 waveform = torchaudio.functional.resample(
    #                     waveform, sr, 16000
    #                 )
    #             speeches.append(waveform.numpy())

    #         inputs = self.processor(
    #             speeches,
    #             sampling_rate=16000,
    #             return_tensors="pt",
    #             # padding=True
    #             padding="max_length",
    #             max_length=1500,
    #             truncation=True
    #         ).input_features.to(self.device)

    #         with torch.no_grad():
    #             with autocast(device_type="cuda", enabled=self.device == "cuda"):
    #                 predicted_ids = self.model.generate(
    #                     inputs,
    #                     forced_decoder_ids=forced_decoder_ids
    #                 )

    #         texts = self.processor.batch_decode(
    #             predicted_ids,
    #             skip_special_tokens=True
    #         )

    #         results.extend(texts)

    #     return results
    
    def batch_transcribe(self, audio_paths):
        import torch
        import torchaudio
        import torch.nn.functional as F
        from torch.cuda.amp import autocast
        from tqdm import tqdm

        results = []

        forced_decoder_ids = self.processor.get_decoder_prompt_ids(
            language="vi",
            task="transcribe"
        )

        MAX_MEL_LEN = 3000  # ðŸ”¥ Báº®T BUá»˜C

        for i in tqdm(range(0, len(audio_paths), self.batch_size), desc="Transcribing"):
            batch_paths = audio_paths[i:i + self.batch_size]
            mel_batch = []

            for path in batch_paths:
                waveform, sr = torchaudio.load(path)
                waveform = waveform.mean(dim=0)

                if sr != 16000:
                    waveform = torchaudio.functional.resample(waveform, sr, 16000)

                # ðŸ”¥ Táº O MEL SPECTROGRAM
                mel = self.processor.feature_extractor(
                    waveform.numpy(),
                    sampling_rate=16000,
                    return_tensors="pt"
                ).input_features[0]   # (80, T)

                # ðŸ”¥ PAD / TRUNCATE MEL â†’ 3000
                T = mel.shape[1]
                if T < MAX_MEL_LEN:
                    mel = F.pad(mel, (0, MAX_MEL_LEN - T))
                else:
                    mel = mel[:, :MAX_MEL_LEN]

                mel_batch.append(mel)

            inputs = torch.stack(mel_batch).to(self.device)  # (B, 80, 3000)

            # ðŸ”’ ASSERT CHá»NG CRASH
            assert inputs.shape[-1] == 3000, inputs.shape

            with torch.no_grad():
                with autocast(enabled=self.device == "cuda"):
                    predicted_ids = self.model.generate(
                        input_features=inputs,
                        forced_decoder_ids=forced_decoder_ids
                    )

            texts = self.processor.batch_decode(
                predicted_ids,
                skip_special_tokens=True
            )

            results.extend(texts)

        return results

    def generate_dataframe(self) -> pd.DataFrame:
        # ===== scare & surprise =====
        sca_folder = os.path.join(self.dataset_path, "scare")
        sup_folder = os.path.join(self.dataset_path, "surprise")

        sca_df = self.labeling_data(sca_folder, "scare")
        sup_df = self.labeling_data(sup_folder, "surprise")

        # ===== happy + neutral + angry + sad =====
        metadata_path = os.path.join(
            self.dataset_path,
            "happy+neutral+angry+sad",
            "metadata.csv"
        )

        meta_df = pd.read_csv(metadata_path)


        base_path = os.path.join(
            self.dataset_path,
            "happy+neutral+angry+sad",
            "converted"
        )

        meta_df["audio"] = meta_df["filename"].apply(
            lambda x: os.path.join(base_path, x)
        )

        meta_df = meta_df[["audio", "emotion"]]

        # ===== Combine =====
        data_df = pd.concat(
            [sca_df, sup_df, meta_df],
            ignore_index=True
        )

        print(f"[INFO] Total audio files: {len(data_df)}")

        # ===== Transcribe =====
        texts = self.batch_transcribe(data_df["audio"].tolist())

        data_df["text"] = texts

        # ===== Shuffle =====
        data_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)


        return data_df

    # inference
    def generate_inference_dataframe(self) -> pd.DataFrame:
        import glob

        audio_paths = glob.glob(
            os.path.join(self.inference_dataset_path, ".wav")
        )

        print(f"[INFO] Total inference audio files: {len(audio_paths)}")

        texts = self.batch_transcribe(audio_paths)

        data_df = pd.DataFrame({
            "audio": audio_paths,
            "text": texts
        })

        return data_df